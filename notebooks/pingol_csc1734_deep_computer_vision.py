# -*- coding: utf-8 -*-
"""Pingol - CSC1734 Deep Computer Vision

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1R4Y4Gq2Ib_rYcOxce-YCMZyj5PSWBKH4
"""

import os
import kagglehub
import numpy as np
import tensorflow as tf
import matplotlib.pyplot as plt

from tensorflow.keras.preprocessing.image import ImageDataGenerator
from tensorflow.keras.applications import ResNet50
from tensorflow.keras.layers import Dense, GlobalAveragePooling2D
from tensorflow.keras.models import Model

from sklearn.utils import class_weight

# ------------------
# CONFIGURATION
# ------------------
IMAGE_SIZE = (224, 224)
BATCH_SIZE = 16
NUM_CLASSES = 6
DATASET_PATH = "TrashType_Image_Dataset"   # Root dataset folder

# ---------------------
# 1. DOWNLOAD DATASET
# ---------------------
path = kagglehub.dataset_download(
    "farzadnekouei/trash-type-image-dataset"
)

print("Dataset downloaded to:", path)
print("Folder contents:", os.listdir(path))

# Adjust this if needed
DATASET_PATH = os.path.join(path, "TrashType_Image_Dataset")
print("Dataset path to:", DATASET_PATH)

# ---------------------
# 2. CONFIG
# ---------------------
IMAGE_SIZE = (224,224)
BATCH_SIZE = 16
NUM_CLASSES = 6

# ---------------------
# 3. DATA GENERATORS
# ---------------------
datagen = ImageDataGenerator(
    rescale=1./255,
    validation_split=0.2,
    rotation_range=15,
    zoom_range=0.1,
    horizontal_flip=True
)

train_data = datagen.flow_from_directory(
    DATASET_PATH,
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    subset="training",
    class_mode="categorical"
)

val_data = datagen.flow_from_directory(
    DATASET_PATH,
    target_size=IMAGE_SIZE,
    batch_size=BATCH_SIZE,
    subset="validation",
    class_mode="categorical"
)

print("Classes:", train_data.class_indices)

# ---------------------
# 4. CLASS WEIGHTS
# ---------------------
weights = class_weight.compute_class_weight(
    class_weight="balanced",
    classes=np.unique(train_data.classes),
    y=train_data.classes
)

class_weights = dict(enumerate(weights))

print("Class weights:", class_weights)

# ---------------------
# 5. BUILD RESNET MODEL
# ---------------------
base_model = ResNet50(
    weights="imagenet",
    include_top=False,
    input_shape=(224,224,3)
)

for layer in base_model.layers:
    layer.trainable = False

x = base_model.output
x = GlobalAveragePooling2D()(x)
x = Dense(128, activation="relu")(x)
output = Dense(NUM_CLASSES, activation="softmax")(x)

model = Model(inputs=base_model.input, outputs=output)

model.compile(
    optimizer="adam",
    loss="categorical_crossentropy",
    metrics=["accuracy"]
)

model.summary()

# ---------------------
# 6. TRAINING
# ---------------------
history = model.fit(
    train_data,
    validation_data=val_data,
    epochs=20,
    class_weight=class_weights
)

# ---------------------
# 7. SAVE MODEL
# ---------------------
model.save("resnet_garbage_classifier.h5")
print("Model saved!")

# ---------------------
# 8. VISUALIZATION
# ---------------------

# Accuracy chart
plt.figure()
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('Model Accuracy')
plt.xlabel('Epoch')
plt.ylabel('Accuracy')
plt.legend(['Train', 'Validation'])
plt.grid(True)
plt.savefig("accuracy_curve.png")
plt.show()

# Loss chart
plt.figure()
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('Model Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.legend(['Train', 'Validation'])
plt.grid(True)
plt.savefig("loss_curve.png")
plt.show()

train_acc = history.history['accuracy'][-1]
val_acc = history.history['val_accuracy'][-1]

train_loss = history.history['loss'][-1]
val_loss = history.history['val_loss'][-1]

print("\nFINAL RESULTS")
print("-----------------")
print(f"Train Accuracy: {train_acc:.3f}")
print(f"Validation Accuracy: {val_acc:.3f}")
print(f"Train Loss: {train_loss:.3f}")
print(f"Validation Loss: {val_loss:.3f}")

from sklearn.metrics import confusion_matrix, classification_report
import seaborn as sns

# Get predictions
val_preds = model.predict(val_data)
y_pred = np.argmax(val_preds, axis=1)
y_true = val_data.classes

# Confusion Matrix
cm = confusion_matrix(y_true, y_pred)

plt.figure(figsize=(8,6))
sns.heatmap(cm, annot=True, cmap="Blues",
            xticklabels=val_data.class_indices,
            yticklabels=val_data.class_indices)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix")
plt.savefig("confusion_matrix.png")
plt.show()

# Classification metrics
print("\nClassification Report:\n")
print(classification_report(y_true, y_pred,
      target_names=val_data.class_indices.keys()))
